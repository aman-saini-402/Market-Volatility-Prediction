{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport lightgbm as lgb\nfrom lightgbm import plot_importance\nfrom joblib import Parallel, delayed\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.preprocessing import MaxAbsScaler\nimport warnings\nwarnings.filterwarnings('ignore')\nseed = 21\nnp.random.seed(seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-28T14:03:17.542564Z","iopub.execute_input":"2021-08-28T14:03:17.542949Z","iopub.status.idle":"2021-08-28T14:03:20.289857Z","shell.execute_reply.started":"2021-08-28T14:03:17.542899Z","shell.execute_reply":"2021-08-28T14:03:20.288782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook is inspired from these very good notebooks and I request everyone to check them out:\n\n[Notebook 1](https://www.kaggle.com/ragnar123/optiver-realized-volatility-lgbm-baseline)\n\n[Notebook 2](https://www.kaggle.com/somang1418/tuning-hyperparameters-under-10-minutes-lgbm)\n\n[Notebook 3](https://www.kaggle.com/felipefonte99/optiver-lgb-with-optimized-params)\n\nI have optimized the code and tried to make it more beginner friendly (hence for myself)","metadata":{}},{"cell_type":"code","source":"base_dir = \"../input/optiver-realized-volatility-prediction/\"","metadata":{"execution":{"iopub.status.busy":"2021-08-28T14:03:20.291483Z","iopub.execute_input":"2021-08-28T14:03:20.291804Z","iopub.status.idle":"2021-08-28T14:03:20.296383Z","shell.execute_reply.started":"2021-08-28T14:03:20.291762Z","shell.execute_reply":"2021-08-28T14:03:20.295185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"# Function to calculate wap\ndef calc_wap(bid_price, ask_price, bid_size, ask_size):\n    wap = ((bid_price*ask_size) + (ask_price*bid_size))/(bid_size+ask_size)\n    return wap\n\n\n# Function to calculate log return\ndef calc_lr(wap_series):\n    return np.log(wap_series).diff()\n\n\n# Function to calculate realized volatility\ndef realized_volatility(log_returns):\n    return np.sqrt(np.sum(log_returns**2))\n\n\n# Function to calculate number of unique values\ndef unique_len(series):\n    return len(series.unique())\n\n\n# Function to calculate Bollinger bands\ndef upper_bb(prices):\n    mean = prices.mean()\n    std = prices.std()\n    return mean + std\ndef middle_bb(prices):\n    return prices.mean()\ndef lower_bb(prices):\n    mean = prices.mean()\n    std = prices.std()\n    return mean - std\n\n\n# Functions to calculate Donchian Channels\ndef upper_dc(prices):\n    return prices.max()\ndef lower_dc(prices):\n    return prices.min()\ndef middle_dc(prices):\n    max_p = prices.max()\n    min_p = prices.min()\n    return (max_p-min_p)/2\n\n\n# Function to calculate Ichimoku Clouds\ndef ichimoku(prices):\n    cl = (1200 - prices.max() - prices.min())/2\n    bl = (2 - prices.max() - prices.min())/2\n    lsa = (cl + bl)/2\n    lsb = (4 - prices.max() - prices.min())/2\n    return lsa - lsb","metadata":{"execution":{"iopub.status.busy":"2021-08-28T14:03:20.29866Z","iopub.execute_input":"2021-08-28T14:03:20.299103Z","iopub.status.idle":"2021-08-28T14:03:20.314058Z","shell.execute_reply.started":"2021-08-28T14:03:20.299058Z","shell.execute_reply":"2021-08-28T14:03:20.312766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_window_features(df, nWindows, is_book=True):\n    \"\"\"Aggreagate window statistics for book and trade data with different\n        window sizes.\"\"\"\n    if is_book:\n        agg_dict = {\n            \"wap1\": [\"mean\", \"sum\", \"std\"],\n            \"wap2\": [\"mean\", \"sum\", \"std\"],\n            \"spread\": [\"mean\", \"std\", \"sum\"],\n            \"bid_spread\": [\"mean\", \"std\", \"sum\"],\n            'ask_spread': [\"mean\", \"std\", 'sum'],\n            \"log_return1\": [\"mean\", \"std\", realized_volatility, 'sum'],\n            \"log_return2\": [\"mean\", \"std\", realized_volatility, \"sum\"],\n            \"total_volume\": [\"mean\", \"sum\", \"std\"],\n            \"volume_imbalance\": [\"mean\", \"std\", \"sum\"]\n        }\n    \n    else:\n        agg_dict = {\n            \"price\": [upper_bb, middle_bb, lower_bb, upper_dc, middle_dc, lower_dc, ichimoku, realized_volatility],\n            \"size\" : [\"mean\", \"std\", \"sum\"],\n            \"order_count\": [\"median\"],\n            \"seconds_in_bucket\": [unique_len]\n        }\n    \n    df_main = df.groupby(\"time_id\").agg(agg_dict).reset_index()\n    # rename columns to avoid multi-level indexing\n    df_main.columns = [\"_\".join(col) for col in df_main.columns]\n    for i in np.linspace(0, 599, nWindows+1)[1:-1]:\n        df_grp = df[df['seconds_in_bucket'] >= i].groupby(\"time_id\").agg(agg_dict).reset_index()\n        df_grp.columns = [\"_\".join(col)+f\"_{int(i)}\" for col in df_grp.columns]\n        # merge with the main dataframe\n        df_main = df_main.merge(df_grp, how=\"left\", left_on=\"time_id_\", right_on=f\"time_id__{int(i)}\")\n        # remove unnecessary time_id columns\n        df_main = df_main.drop(f\"time_id__{int(i)}\", axis=1)\n    \n    return df_main","metadata":{"execution":{"iopub.status.busy":"2021-08-28T14:03:20.315736Z","iopub.execute_input":"2021-08-28T14:03:20.316276Z","iopub.status.idle":"2021-08-28T14:03:20.333531Z","shell.execute_reply.started":"2021-08-28T14:03:20.316244Z","shell.execute_reply":"2021-08-28T14:03:20.332279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_book(stock_id, nWindows, is_train=True):\n    \"\"\"Preprocess book data one stock id at a time\"\"\"\n    \n    if is_train:\n        df = pd.read_parquet(base_dir + f\"book_train.parquet/stock_id={stock_id}\")\n    else:\n        df = pd.read_parquet(base_dir + f\"book_test.parquet/stock_id={stock_id}\")\n    # Extract basic granular features\n    df['wap1'] = calc_wap(df['bid_price1'], df['ask_price1'], df['bid_size1'], df['ask_size1'])\n    df['wap2'] = calc_wap(df['bid_price2'], df['ask_price2'], df['bid_size2'], df['ask_size2'])\n    df['spread'] = (df['ask_price1'] - df['bid_price1']) + (df['ask_price2'] - df['bid_price2'])\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price2'] - df['ask_price1']\n    df['log_return1'] = calc_lr(df['wap1'])\n    df['log_return2'] = calc_lr(df['wap2'])\n    df['total_volume'] = df['bid_size1'] + df['bid_size2'] + df['ask_size1'] + df['ask_size2']\n    df['volume_imbalance'] = (df['ask_size1']+df['ask_size2'])-(df[\"bid_size1\"]+df['bid_size2'])\n    \n    # Calculate window statistics\n    df = calc_window_features(df, nWindows, is_book=True)\n    \n    # create row_id colume to facilitate merging\n    df['row_id'] = str(stock_id) + \"-\" + df['time_id_'].astype(\"str\")\n    \n    # remove time_id column to avoid redundancy while merging\n    df = df.drop(\"time_id_\", axis=1)\n    \n    return df\n\n\ndef preprocess_trade(stock_id, nWindows, is_train=True):\n    \"\"\"Preprocess trade data one stokc id at a time\"\"\"\n    \n    if is_train:\n        df = pd.read_parquet(base_dir + f\"trade_train.parquet/stock_id={stock_id}\")\n    else:\n        df = pd.read_parquet(base_dir + f\"trade_test.parquet/stock_id={stock_id}\")\n    \n    # Add granular features here\n    \n    # Calculate window statistics\n    df = calc_window_features(df, nWindows, is_book=False)\n    \n    # create row_id colume to facilitate merging\n    df['row_id'] = str(stock_id) + \"-\" + df['time_id_'].astype(\"str\")\n    \n    # remove time_id column to avoid redundancy while merging\n    df = df.drop(\"time_id_\", axis=1)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-28T14:03:20.866725Z","iopub.execute_input":"2021-08-28T14:03:20.867229Z","iopub.status.idle":"2021-08-28T14:03:20.88143Z","shell.execute_reply.started":"2021-08-28T14:03:20.867198Z","shell.execute_reply":"2021-08-28T14:03:20.880059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(stock_id_list, nWindows, is_train=True):\n    \"\"\"Parallel preprocessing of book and trade data\"\"\"\n    \n    # Parallel processing loop function\n    def for_joblib(stock_id):\n        \n        # Preprocess book and trade data and merge them\n        df_per_stock = pd.merge(preprocess_book(stock_id,nWindows,is_train),\n                               preprocess_trade(stock_id, nWindows,is_train),\n                               on=\"row_id\",\n                               how=\"left\")\n        return df_per_stock\n    \n    # Parallel API\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in stock_id_list)\n    # Concatenate all stock's feature data together\n    df = pd.concat(df, ignore_index=True)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-28T14:03:21.317317Z","iopub.execute_input":"2021-08-28T14:03:21.317855Z","iopub.status.idle":"2021-08-28T14:03:21.324119Z","shell.execute_reply.started":"2021-08-28T14:03:21.317822Z","shell.execute_reply":"2021-08-28T14:03:21.323158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def stock_and_time_stats(df):\n    \"\"\"Calculate group statistics for volatility columns per stock_id and per time_id\"\"\"\n    \n    volatility_cols = df.filter(regex=\"_realized_volatility\").columns\n    \n    # Group by stock id\n    df_stock_id = df.groupby(\"stock_id\")[volatility_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by time id\n    df_time_id = df.groupby(['time_id'])[volatility_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n\n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    # Drop redundant columns\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-28T14:03:22.363519Z","iopub.execute_input":"2021-08-28T14:03:22.363896Z","iopub.status.idle":"2021-08-28T14:03:22.373328Z","shell.execute_reply.started":"2021-08-28T14:03:22.363868Z","shell.execute_reply":"2021-08-28T14:03:22.372068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_kmeans(train, test=None):\n    \"\"\"Apply kmeans clustering to model stocks in different sectors\"\"\"\n    \n    train_vol = train.filter(regex=\"_realized_volatility\")\n    train_vol['stock_id'] = train['stock_id']\n    # test_vol = test.filter(regex=\"_realized_volatility\")\n    # test_vol['stock_id'] = test['stock_id']\n    \n    # scale the data\n    scaler = MaxAbsScaler()\n    train_scaled = scaler.fit_transform(pd.get_dummies(train_vol, columns=[\"stock_id\"]))\n    print(np.sum(np.isnan(train_scaled)))\n    train_scaled = np.nan_to_num(train_scaled)\n    # test_scaled = scaler.transform(pd.get_dummies(test_vol, columns=['stock_id']))\n    # Test data have some nan values (only 0.001% of the total data)\n    # test_scaled = np.nan_to_num(test_scaled)\n\n    \n    # Apply kmeans\n    kmean = MiniBatchKMeans(n_clusters=11, batch_size=128)\n    \n    # Predict sectors\n    train_sectors = kmean.fit_predict(train_scaled)\n    # test_sectors = kmean.predict(test_scaled)\n    train['sector'] = train_sectors\n    # test['sector'] = test_sectors\n    \n    # Group by sector\n    volatility_cols = train_vol.columns\n    train_sector = train.groupby(\"sector\")[volatility_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # test_sector = test.groupby(\"sector\")[volatility_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    train_sector.columns = ['_'.join(col) for col in train_sector.columns]\n    train_sector = train_sector.add_suffix('_' + 'sector')\n    \n    # test_sector.columns = ['_'.join(col) for col in test_sector.columns]\n    # test_sector = test_sector.add_suffix('_' + 'sector')\n    \n    # Merge with original dataframe\n    train = train.merge(train_sector, how = 'left', left_on = ['sector'], right_on = ['sector__sector'])\n    # test = test.merge(test_sector, how = 'left', left_on = ['sector'], right_on = ['sector__sector'])\n    \n    # Drop redundant columns\n    train.drop(['sector__sector'], axis = 1, inplace = True)\n    # test.drop(['sector__sector'], axis = 1, inplace = True)\n    \n    return train# , test","metadata":{"execution":{"iopub.status.busy":"2021-08-28T14:07:32.345189Z","iopub.execute_input":"2021-08-28T14:07:32.345737Z","iopub.status.idle":"2021-08-28T14:07:32.355151Z","shell.execute_reply.started":"2021-08-28T14:07:32.345706Z","shell.execute_reply":"2021-08-28T14:07:32.35434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\n\ndef train_and_evaluate(train, test, lgb_params, FOLDS, predict=True):\n    \n    # Prepare Input Data\n    groups = train['time_id']\n    x = train.drop([\"row_id\", \"time_id\", \"target\"], axis= 1)\n    y = train['target']\n    x_test = test.drop([\"row_id\", \"time_id\"], axis = 1)\n    \n    # Transform stock id to numeric column\n    x['stock_id'] = x['stock_id'].astype(\"int\")\n    x_test['stock_id'] = x_test['stock_id'].astype(\"int\")\n    \n    # Prediction arrays\n    val_pred = np.zeros(x.shape[0])\n    test_pred = np.zeros(x_test.shape[0])\n    train_rmspe = 0\n    \n    # K-fold cross valiation\n    kfold = GroupKFold(n_splits=FOLDS)\n    for fold, (train_idx, val_idx) in enumerate(kfold.split(x, y, groups)):\n        print(\"#\"*40)\n        print(\"\\nTraining Fold - \", fold+1)\n        X_train, y_train = x.iloc[train_idx], y.iloc[train_idx]\n        X_val, y_val     = x.iloc[val_idx]  , y.iloc[val_idx]\n        \n        # Add weights here\n        train_weights = 1 / np.square(y_train)\n        val_weights = 1 / np.square(y_val)\n        \n        # create lgb datasets\n        train_data = lgb.Dataset(X_train, y_train, weight = train_weights, categorical_feature = [\"stock_id\", \"sector\"])\n        val_data   = lgb.Dataset(X_val,   y_val, weight = val_weights,   categorical_feature = [\"stock_id\", \"sector\"])\n        \n        # Train model\n        model = lgb.train(params = lgb_params,\n                         train_set = train_data,\n                         valid_sets = [train_data, val_data],\n                         num_boost_round = 9999999,\n                         early_stopping_rounds = 70,\n                         feval = feval_rmspe,\n                         valid_names=[\"training\", \"validation\"],\n                         # callbacks=[neptune_callback],\n                         verbose_eval = 200)\n        \n        # Plot feature importance\n        plot_importance(model, max_num_features=20, figsize= (12, 6))\n        plt.show()\n        \n        # Predict on train set\n        train_rmspe += rmspe(y_train, model.predict(X_train)) / FOLDS\n        # Predict validation set\n        val_pred[val_idx] = model.predict(X_val)\n        # Predict test set\n        test_pred += model.predict(x_test) / FOLDS\n        \n    return val_pred, test_pred, train_rmspe","metadata":{"execution":{"iopub.status.busy":"2021-08-28T14:03:25.841428Z","iopub.execute_input":"2021-08-28T14:03:25.841789Z","iopub.status.idle":"2021-08-28T14:03:25.861034Z","shell.execute_reply.started":"2021-08-28T14:03:25.841759Z","shell.execute_reply":"2021-08-28T14:03:25.859846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install neptune-client\n# !pip install neptune-lightgbm\n# !pip install psutil\n\n# import neptune.new as neptune\n# from neptune.new.integrations.lightgbm import NeptuneCallback\n# from neptune.new.types import File\n\n# # Neptune Setup\n# # Create run\n# run = neptune.init(\n#     project=\"amansaini150/Optiver\",\n#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmMTFkNjI2MS04ZTg0LTQ3YTMtYjNiZi02OGMzNmQ3ODQyZDMifQ==\",\n#     tags=[\"lgbm-integration\", \"GroupKFold\", \"reg\", \"group-stats\", \"weights\", \"kmeans\"]\n# )\n# # Create neptune callback\n# neptune_callback = NeptuneCallback(run=run)\n\n# # Pass Neptune in the callback","metadata":{"execution":{"iopub.status.busy":"2021-08-28T13:16:54.558343Z","iopub.execute_input":"2021-08-28T13:16:54.558915Z","iopub.status.idle":"2021-08-28T13:16:54.579998Z","shell.execute_reply.started":"2021-08-28T13:16:54.558878Z","shell.execute_reply":"2021-08-28T13:16:54.57868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training and Testing\n\n# Parameters\nnWindows = 4\nFOLDS = 5\nlgb_params = {\n        'learning_rate': 0.13572437900113307,        \n        'lambda_l1': 2.154360665259325,\n        'lambda_l2': 6.711089761523827,\n        'num_leaves': 769,\n        'min_sum_hessian_in_leaf': 20.44437160769411,\n        'feature_fraction': 0.7921473067441019,\n        'feature_fraction_bynode': 0.8083803860191322,\n        'bagging_fraction': 0.9726755660563261,\n        'bagging_freq': 42,\n        'min_data_in_leaf': 690,\n        'max_depth': 3,\n        'seed': seed,\n        'feature_fraction_seed': seed,\n        'bagging_seed': seed,\n        'drop_seed': seed,\n        'data_random_seed': seed,\n        'objective': 'rmse',\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs': -1,\n}\n\n# Load target datasets\ntrain = pd.read_csv(base_dir + \"train.csv\")\ntest = pd.read_csv(base_dir + \"test.csv\")\n\n# Preprocess feature data for train\ntrain['row_id'] = train['stock_id'].astype(\"str\") + \"-\" + train['time_id'].astype(\"str\")\ntrain_feature = preprocess(train['stock_id'].unique(), nWindows, is_train=True)\ntrain = train.merge(train_feature, on=\"row_id\", how=\"left\")\n\n# Preprocess feature data for test\ntest_feature = preprocess(test['stock_id'].unique(), nWindows, is_train=False)\ntest = test.merge(test_feature, on=\"row_id\", how=\"left\")\n\n# Calculate group statistics\ntrain = stock_and_time_stats(train)\ntest = stock_and_time_stats(test)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T14:03:31.526894Z","iopub.execute_input":"2021-08-28T14:03:31.527263Z","iopub.status.idle":"2021-08-28T14:03:35.106065Z","shell.execute_reply.started":"2021-08-28T14:03:31.527234Z","shell.execute_reply":"2021-08-28T14:03:35.103295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test[['row_id']].to_csv('submission.csv',index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/optiverdatasetwithoutkmeans/training_data.pkl\", \"rb\") as f:\n    train = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T14:05:22.002279Z","iopub.execute_input":"2021-08-28T14:05:22.002699Z","iopub.status.idle":"2021-08-28T14:05:29.351616Z","shell.execute_reply.started":"2021-08-28T14:05:22.002665Z","shell.execute_reply":"2021-08-28T14:05:29.350546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply Kmeans\n# train, test = apply_kmeans(train, test)\ntrain = apply_kmeans(train)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T14:07:37.969237Z","iopub.execute_input":"2021-08-28T14:07:37.969786Z","iopub.status.idle":"2021-08-28T14:09:00.829467Z","shell.execute_reply.started":"2021-08-28T14:07:37.969753Z","shell.execute_reply":"2021-08-28T14:09:00.828415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save training and testing data for Tuning\nwith open(\"training_data.pkl\", \"wb\") as f:\n    pickle.dump(train, f)\n# with open(\"testing_data.pkl\", \"wb\") as f:\n#     pickle.dump(test, f)\n# Load saved data\n# with open(\"../input/optiver-processed-data/training_data.pkl\", \"rb\") as f:\n#     train = pickle.load(f)\n# with open(\"../input/optiver-processed-data/testing_data.pkl\", \"rb\") as f:\n#     test = pickle.load(f)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-28T14:09:12.947861Z","iopub.execute_input":"2021-08-28T14:09:12.948237Z","iopub.status.idle":"2021-08-28T14:09:15.845153Z","shell.execute_reply.started":"2021-08-28T14:09:12.948208Z","shell.execute_reply":"2021-08-28T14:09:15.844106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nsns.barplot(x = train['sector'].value_counts().index, y=train['sector'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:21:48.303099Z","iopub.execute_input":"2021-08-24T16:21:48.303397Z","iopub.status.idle":"2021-08-24T16:21:48.913567Z","shell.execute_reply.started":"2021-08-24T16:21:48.303369Z","shell.execute_reply":"2021-08-24T16:21:48.912451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and Predict\nval_pred, test_pred, train_rmspe = train_and_evaluate(train, test, lgb_params, FOLDS)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:21:48.916287Z","iopub.execute_input":"2021-08-24T16:21:48.916754Z","iopub.status.idle":"2021-08-24T16:34:14.596141Z","shell.execute_reply.started":"2021-08-24T16:21:48.916706Z","shell.execute_reply":"2021-08-24T16:34:14.595055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # log parameters into Neptune\n# run[\"parameters\"] = lgb_params\n# run[\"parameters/folds\"] = FOLDS\n# run[\"parameters/nWindows\"] = nWindows\n# run[\"oof_score\"] = rmspe(train['target'], val_pred)\n# run[\"train_score\"] = train_rmspe\n# run.stop()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:34:14.598052Z","iopub.execute_input":"2021-08-24T16:34:14.5984Z","iopub.status.idle":"2021-08-24T16:34:15.099726Z","shell.execute_reply.started":"2021-08-24T16:34:14.598365Z","shell.execute_reply":"2021-08-24T16:34:15.09881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Prepare submission\ntest['target'] = test_pred\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:34:15.101116Z","iopub.execute_input":"2021-08-24T16:34:15.101462Z","iopub.status.idle":"2021-08-24T16:34:15.111595Z","shell.execute_reply.started":"2021-08-24T16:34:15.101427Z","shell.execute_reply":"2021-08-24T16:34:15.110099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}